[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to the new, quarto-based version of Statisfaction",
    "section": "",
    "text": "Hey! We have just moved this blog from Wordpress to github. The old version is still available here. The new version is based on quarto, which will make it much easier to write mathematics, e.g. \\(\\pi(\\theta|x) \\propto \\pi(\\theta) L(x|\\theta)\\), and code, e.g. \nimport numpy as np\n\ndef fact(n):\n    return np.prod(range(1, n + 1))"
  },
  {
    "objectID": "posts/01-09-2023-particles-v0.4/index.html",
    "href": "posts/01-09-2023-particles-v0.4/index.html",
    "title": "particles version 0.4: single-run variance estimates, FFBS variants, nested sampling",
    "section": "",
    "text": "Version 0.4 of particles have just been released. Here are the main changes:\n\nSingle-run variance estimation for waste-free SMC\nWaste-free SMC (Dau & Chopin, 2020) was already implemented in particles (since version 0.3), and even proposed by default. This is a variant of SMC samplers where you resample only \\(M \\ll N\\) particles, apply to each resampled particle \\(P-1\\) MCMC steps, and then gather these \\(M\\times P\\) states to form the next particle sample; see the paper if you want to know why this is a good idea (short version: this tends to perform better than standard SMC samplers, and to be more robust to the choice of the number of MCMC steps).\nWhat was not yet implemented (but is, in this version) is the single-run variance estimates proposed in the same paper. Here is a simple illustration:\n\n\n\n\n\n\n\n\n\n\nBoth plots were obtained from \\(10^3\\) runs of waste-free IBIS (i.e. target at time \\(t\\) is the posterior based on the first \\(t+1\\) observations, \\(p(\\theta|y_{0:t})\\)) applied to Bayesian logistic regression and the Pima Indians dataset. The red line is the empirical variance of the output, and, since the number of runs is large, it should be close to the true variance. The lower (resp. upper) limit of the grey area is the \\(5\\%\\) (resp. \\(95\\%\\)) quantile of the single-run variance estimates obtained from these \\(10^3\\) runs. The considered output is either the posterior mean of the intercept (top) or the log marginal likelihood (bottom).\nWe can see from these plots that these single-run estimates are quite reliable, and make it possible, in case one uses IBIS, to obtain error bars even from a single run. See the documention of module smc_samplers (or the scripts in papers/wastefreeSMC) for more details on how you may get such estimates.\n\n\nNew FFBS variants\nI have already mentioned in a previous post, on the old blog, that particles now implement new FFBS algorithms (i.e. particle smoothing algorithms that rely on a backward step) that were proposed in this paper. On top of that, particles now also includes a hybrid version of the Paris algorithm.\n\n\nNested sampling\nI was invited to this nested sampling workshop in Munich, so this gave me some incentive to:\n\nclean up and document the “vanilla” nested sampling implementation which was in module nested.\nadd to the same module the NS-SMC samplers of Salomone et al (2018) to play with them and do some numerical experiments to illustrate my talk.\n\nI will blog shortly about the interesting results I found (which essentiaZZlly are in line with Salmone et al).\n\n\nOther minor changes\nSeveral distributions and a dataset (Liver) were added, see the change log.\n\n\nLogo\nI’ve added a logo. It’s… not great, if anyone has suggestions on how to design a better log, I am all ears.\n\n\nWhat’s next?\nI guess what’s still missing from the package are stuff like:\n\nthe ensemble Kalman filter, which would be reasonably easy to add, and would be useful in various problems;\nadvanced methods to design better proposals, such as controlled SMC (Heng et al, 2020) or the iterated auxiliary particle filter (Guarniero et al, 2017).\n\nIf you have other ideas, let me know."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statisfaction - a collaborative blog about Bayesian computation and related topics",
    "section": "",
    "text": "particles version 0.4: single-run variance estimates, FFBS variants, nested sampling\n\n\n\n\n\n\n\nnews\n\n\nparticles\n\n\nSMC\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\n  \n\n\n\n\nBetter than Monte Carlo (this post is not about QMC)\n\n\n\n\n\n\n\nMonte Carlo\n\n\nQMC\n\n\nrates\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to the new, quarto-based version of Statisfaction\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog initially started, a long time ago, by PhD students at ENSAE interested in Bayesian computation and related topics. Used to be hosted on Wordpress here, but now lives a happier life on github."
  },
  {
    "objectID": "posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html",
    "href": "posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html",
    "title": "Better than Monte Carlo (this post is not about QMC)",
    "section": "",
    "text": "(This is repost from this December 2022 post on the old website, but since math support is so poor on Wordpress, I’d rather have this post published here.)\nSay I want to approximate the integral \\[I(f) := \\int_{[0, 1]^s} f(u) du\\] based on \\(n\\) evaluations of function \\(f\\). I could use plain old Monte Carlo: \\[\\hat{I}(f) = \\frac 1 n \\sum_{i=1}^n f(U_i),\\quad U_i \\sim \\mathrm{U}([0,\n1]^s).\\] whose RMSE (root mean square error) is \\(O(n^{-1/2})\\).\nCan I do better? That is, can I design an alternative estimator/algorithm, which performs \\(n\\) evaluations and returns a random output, such that its RMSE converge quicker?\nSurprisingly, the answer to this question has been known for a long time. If I am ready to focus on functions \\(f\\in\\mathcal{C}^r([0, 1]^s)\\), Bakhvalov (1959) showed that the best rate I can hope for is \\(O(n^{-1/2-r/s}).\\) That is, there exist algorithms that achieve this rate, and algorithms achieving a better rate simply do not exist.\nOk, but how can I actually design such an algorithm? The proof of Bakhvalov contains a very simple recipe. Say I am able to construct a good approximation \\(f_n\\) of \\(f\\), based on \\(n\\) evaluations; assume the approximation error is \\(\\|f-f_n\\|_\\infty = O(n^{-\\alpha})\\), \\(\\alpha&gt;0\\). Then I could compute the following estimator, based on a second batch of \\(n\\) evaluations: \\[ \\hat{I}(f)\n:= I(f_n) +  \\frac 1 n \\sum_{i=1}^n (f-f_n)(U_i),\\quad U_i \\sim\n\\mathrm{Uniform}([0, 1]^s).\\] and it is easy to check that this new estimator is unbiased, that its variance is \\(O(n^{-1-2\\alpha})\\), and therefore its RMSE is \\(O(n^{-1/2-\\alpha})\\). (It is based on \\(2n\\) evaluations.)\nSo there is strong relation between Bakhvalov results and function approximation. In fact, the best rate you can achieve for the latter is \\(\\alpha=r/s\\), which explain the rate above for stochastic quadrature. You can see now why I gave this title to this post. QMC is about using points that are better than random points. But here I’m using IID points, and the improved rate comes from the fact I use a better approximation of \\(f\\).\nHere is a simple example of a good function approximation. Take \\(s=1\\), and \\[\nf_n(u) = \\sum_{i=1}^n f( \\frac{2i-1}{2n} ) \\mathbf{1}_{[(i-1)/n, i/n]}(u);\n\\] that is, split \\([0, 1]\\) into \\(n\\) intervals \\([(i-1)/n, i/n]\\), and approximate \\(f\\) inside a given interval by its value at the centre of the interval. You can quickly check that the approximation error is then \\(O(n^{-1})\\) provided \\(f\\) is \\(C^1\\). So you get a simple recipe to get the optimal rate for \\(s=1\\) and \\(r=1\\).\nIs it possible to generalise this type of construction to any \\(r\\) and any \\(s\\)? The answer is in our recent paper with Mathieu Gerber, which you can find here. You may also want to read Novak (2016), which is a very good entry on stochastic quadrature, and in particular gives a nice overview of Bakhvalov’s and related results."
  }
]